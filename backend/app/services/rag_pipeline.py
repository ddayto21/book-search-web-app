from app.clients.llm_client import DeepSeekAPIClient
from typing import AsyncGenerator, List, Dict
from app.services.preprocessing import preprocess_book
import logging


def generate_book_summaries(books: List[Dict]):
    """
    Generate concise summaries for each book.

    For each book, this function extracts the title, author, year, and the first three subjects,
    and formats them into a summary string using the `preprocess_book` helper.

    Args:
        books (List[Dict]): A list of book metadata dictionaries with keys like "title", "author",
                            "year", and "subjects".

    Returns:
        List[str]: A list of formatted summary strings.

    """
    return [preprocess_book(book) for book in books]


def construct_model_prompt(query: str, retrieved_context: List[str]) -> str:
    """
    Constructs a prompt for the LLM based on the user's query and the retrieved book summaries.

    The prompt instructs the LLM to return exactly 5 book recommendations as a JSON array.
    Each recommendation must be an object with the following keys:
      - title: the book title
      - description: a clear, concise explanation of why the book is relevant to the query

    If none of the retrieved books are relevant, the LLM should generate its own recommendations.
    The output must be strictly a JSON array without any additional text.

    Args:
        query (str): The user's search query.
        book_summaries (List[str]): A list of concise book summaries.

    Returns:
        str: A formatted prompt string.
    """
    prompt_lines = [f"User query: '{query}'.", "", "Retrieved book details:"]

    # Append each book summary with its index.
    for idx, summary in enumerate(retrieved_context):
        prompt_lines.append(f"{idx + 1}. {summary}")

    # Append bullet-pointed instructions.
    prompt_lines.extend(
        [
            "",
            "Instructions:",
            "- Provide a JSON array of book recommendations.",
            "- Each recommendation must be an object with the following keys:",
            "    • title: the book title",
            "    • description: a clear, friendly explanation of why the book is relevant to the query",
            "- If none of the retrieved books match the query, generate your own recommendations.",
            "- Output strictly the JSON array without any additional text.",
        ]
    )

    return "\n".join(prompt_lines)


def construct_messages(prompt: str) -> List[Dict[str, str]]:
    """
    Constructs the conversation messages for the LLM using the given prompt.

    This function returns a list of two messages:
      1. A system message that sets the assistant's role and tone, instructing it to provide clear, accurate book recommendations and explanations.
      2. A user message that contains the prompt generated by the RAG system, which includes the user query, the retrieved book details, and instructions for generating recommendations.

    The format is compatible with mainstream LLM providers like DeepSeek and OpenAI.

    Args:
        prompt (str): The complete prompt string, typically generated by construct_model_prompt, which includes the user's query, a numbered list of book summaries, and detailed instructions.

    Returns:
        List[Dict[str, str]]: A list of message dictionaries with keys "role" and "content".
    """
    return [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant that provides clear and accurate book recommendations and explanations. "
                "Respond in a friendly, concise, and professional manner."
            ),
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]


async def stream_model_response(
    client: DeepSeekAPIClient, messages: List[Dict], temperature: float
) -> AsyncGenerator[str, None]:
    """
    Streams the response from the DeepSeek API as SSE (Server-Sent Events).

    Yields each chunk as an SSE-formatted string and, at the end, yields a final "[DONE]" marker.

    Args:
        llm_client (DeepSeekAPIClient): The LLM client used to query the DeepSeek API.
        messages (List[Dict]): The conversation history to send.
        temperature (float): Controls the randomness of the LLM output.

    Yields:
        str: SSE formatted chunks of text.
    """
    async for chunk in client.async_stream(
        model="deepseek-chat",
        messages=messages,
        temperature=temperature,
    ):
        logging.debug("LLM chunk: %s", chunk)
        # Yield the chunk as an SSE event.
        yield f"data: {chunk}\n\n"
    # Yield final marker in SSE format.
    yield "data: [DONE]\n\n"
