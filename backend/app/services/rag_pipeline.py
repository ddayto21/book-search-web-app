from app.clients.llm_client import DeepSeekAPIClient
from typing import AsyncGenerator, List, Dict
from app.services.preprocessing import preprocess_book
import logging
from fastapi.responses import StreamingResponse


def summarize_context(books: List[Dict]):
    """
    Generate concise summaries for each book.

    For each book, this function extracts the title, author, year, and the first three subjects,
    and formats them into a summary string using the `preprocess_book` helper.

    Args:
        books (List[Dict]): A list of book metadata dictionaries with keys like "title", "author",
                            "year", and "subjects".

    Returns:
        List[str]: A list of formatted summary strings.

    """
    return [preprocess_book(book) for book in books]


def construct_model_prompt(query: str, retrieved_context: List[str]) -> str:
    """
    Constructs a prompt for the LLM based on the user's query and the retrieved book summaries.

    The prompt instructs the LLM to return exactly 5 book recommendations as a JSON array.
    Each recommendation must be an object with the following keys:
      - title: the book title
      - description: a clear, concise explanation of why the book is relevant to the query

    If none of the retrieved books are relevant, the LLM should generate its own recommendations.
    The output must be strictly a JSON array without any additional text.

    Args:
        query (str): The user's search query.
        book_summaries (List[str]): A list of concise book summaries.

    Returns:
        str: A formatted prompt string.
    """
    prompt_lines = [f"User query: '{query}'.", "", "Retrieved book details:"]

    # Append each book summary with its index.
    for idx, summary in enumerate(retrieved_context):
        prompt_lines.append(f"{idx + 1}. {summary}")

    # Append bullet-pointed instructions.
    prompt_lines.extend(
        [
            "",
            "Instructions:",
            "- Provide a JSON array of book recommendations.",
            "- Each recommendation must be an object with the following keys:",
            "    • title: the book title",
            "    • description: a clear, friendly explanation of why the book is relevant to the query",
            "- If none of the retrieved books match the query, generate your own recommendations.",
            "- Output strictly the JSON array without any additional text.",
        ]
    )

    return "\n".join(prompt_lines)


def construct_messages(prompt: str) -> List[Dict[str, str]]:
    """
    Constructs the conversation messages for the LLM using the given prompt.

    This function returns a list of two messages:
      1. A system message that sets the assistant's role and tone, instructing it to provide clear, accurate book recommendations and explanations.
      2. A user message that contains the prompt generated by the RAG system, which includes the user query, the retrieved book details, and instructions for generating recommendations.

    The format is compatible with mainstream LLM providers like DeepSeek and OpenAI.

    Args:
        prompt (str): The complete prompt string, typically generated by construct_model_prompt, which includes the user's query, a numbered list of book summaries, and detailed instructions.

    Returns:
        List[Dict[str, str]]: A list of message dictionaries with keys "role" and "content".
    """
    return [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant that provides clear and accurate book recommendations and explanations. "
                "Respond in a friendly, concise, and professional manner."
            ),
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]


async def sse_response_generator(
    llm_client: DeepSeekAPIClient,
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 1.3,
) -> AsyncGenerator[str, None]:
    """
    Asynchronous generator that streams SSE-formatted responses from the LLM via DeepSeekAPIClient.

    Args:
        llm_client (DeepSeekAPIClient): The API client instance used to call the DeepSeek API.
        model (str): The identifier for the LLM model (e.g., "deepseek-chat").
        messages (List[Dict[str, str]]): A list of message dictionaries representing the conversation history.
        temperature (float, optional): Temperature parameter for the LLM. Defaults to 0.7.

    Yields:
        str: SSE-formatted text chunks as they arrive from the API, with a final marker indicating completion.
    """
    async for chunk in llm_client.async_stream(
        model=model,
        messages=messages,
        temperature=temperature,
    ):
        # Debug output for tracing.
        print(chunk, end="", flush=True)
        # Yield the chunk in SSE format.
        yield f"data: {chunk}\n\n"
    # Yield a final SSE marker to signal the end of the stream.
    yield 'data: {"done": true}\n\n'


# Example usage within FastAPI route:
#
# return StreamingResponse(
#     sse_response_generator(llm_client, "deepseek-chat", messages, 0.7),
#     media_type="text/event-stream"
# )
