# tests/services/test_rag_pipeline.py

import pytest
import re

from app.services.rag_pipeline import (
    summarize_context,
    construct_model_prompt,
    construct_messages,
    sse_response_generator,
)


import os
import pytest
import asyncio


from app.clients.llm_client import DeepSeekAPIClient


def normalize_whitespace(text: str) -> str:
    """
    Normalize whitespace by collapsing multiple spaces/newlines into a single space
    and trimming any leading or trailing whitespace.

    Args:
        text (str): The input text to normalize.

    Returns:
        str: The normalized text.
    """
    return re.sub(r"\s+", " ", text).strip()


def normalize_messages(messages: list) -> list:
    """
    Normalize the 'content' field of each message dictionary by applying whitespace normalization.

    Args:
        messages (list): A list of message dictionaries.

    Returns:
        list: A new list of messages with normalized content.
    """
    return [
        {**msg, "content": normalize_whitespace(msg.get("content", ""))}
        for msg in messages
    ]


def test_summarize_context(retrieved_context_fixture, summaries_fixture):
    """
    Verify that summarize_context correctly extracts and formats summaries from the provided book metadata.

    Args:
        retrieved_context_fixture (list): Fixture providing sample book metadata.
        summaries_fixture (list): Fixture providing the expected book summaries.
    """
    output = summarize_context(retrieved_context_fixture)
    print("Generated Book Summaries:", output)
    assert output == summaries_fixture


def test_construct_model_prompt(query_fixture, summaries_fixture):
    """
    Verify that construct_model_prompt returns a well-formatted prompt that includes:
      - The user query.
      - A numbered list of retrieved book details.
      - Clear instructions for generating recommendations.

    Whitespace differences are normalized prior to comparison.

    Args:
        query_fixture (str): Fixture providing a sample user query.
        summaries_fixture (list): Fixture providing sample book summaries.
    """
    query = query_fixture
    summaries = summaries_fixture

    expected_output = (
        "User query: 'anime similar to hunter hunter'.\n"
        "\n"
        "Retrieved book details:\n"
        "1. hunter x hunter by yoshihiro togashi (1998). Keywords: magic, hunter, graphic novel\n"
        "2. wild by erin hunter (2003). Keywords: cat, fantasy, fantasy fiction\n"
        "3. inuyasha by rumiko takahashi (1998). Keywords: good evil, magic, teenage girl\n"
        "4. long shadow by erin hunter (2008). Keywords: cat, fantasy, fantasy fiction\n"
        "5. forest secret by erin hunter (2003). Keywords: cat, fantasy, fantasy fiction\n"
        "\n"
        "Instructions:\n"
        "- Provide a JSON array of book recommendations.\n"
        "- Each recommendation must be an object with the following keys:\n"
        "    • title: the book title\n"
        "    • description: a clear, friendly explanation of why the book is relevant to the query\n"
        "- If none of the retrieved books match the query, generate your own recommendations.\n"
        "- Output strictly the JSON array without any additional text."
    )

    normalized_expected_output = normalize_whitespace(expected_output)

    actual_output = construct_model_prompt(query, summaries)
    normalized_actual_output = normalize_whitespace(actual_output)

    assert normalized_actual_output == normalized_expected_output


def test_construct_messages(query_fixture, summaries_fixture):
    """
    Verify that construct_messages returns a list with two message dictionaries:
      1. A system message that sets the assistant's role and tone.
      2. A user message containing the prompt generated by construct_model_prompt.

    The comparison normalizes whitespace differences.

    Args:
        query_fixture (str): Fixture providing a sample user query.
        summaries_fixture (list): Fixture providing sample book summaries.
    """
    query = query_fixture
    summaries = summaries_fixture

    prompt = construct_model_prompt(query, summaries)
    expected = [
        {
            "role": "system",
            "content": (
                "You are a helpful assistant that provides clear and accurate book recommendations and explanations. "
                "Respond in a friendly, concise, and professional manner."
            ),
        },
        {
            "role": "user",
            "content": prompt,
        },
    ]

    output = construct_messages(prompt)

    normalized_expected = normalize_messages(expected)

    normalized_output = normalize_messages(output)

    assert normalized_output == normalized_expected


async def test_sse_response_generator_integration(query_fixture, summaries_fixture):
    """
    Integration test for sse_response_generator using a real DeepSeekAPIClient instance.
    This test will:
      - Initialize the client with a valid API key.
      - Send a test prompt.
      - Collect SSE-formatted chunks.
      - Verify that at least one chunk is received and the final marker is present.
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        pytest.skip("No DeepSeek API key provided, skipping integration test.")

    prompt = construct_model_prompt(query_fixture, summaries_fixture)
    messages = construct_messages(prompt)

    client = DeepSeekAPIClient(api_key=api_key)

    model = "deepseek-chat"
    temperature = 0.7

    results = []
    async for chunk in sse_response_generator(client, model, messages, temperature):
        results.append(chunk)

    assert len(results) > 0, "No data received from stream."
    # The final chunk should signal completion.
    final_chunk = results[-1].strip()
    assert (
        final_chunk == 'data: {"done": true}'
    ), "Final marker not received as expected."
